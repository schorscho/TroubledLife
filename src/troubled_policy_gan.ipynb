{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#PROJECT_ROOT_DIR = \"/home/ubuntu/TroubledLife\"\n",
    "PROJECT_ROOT_DIR = \"/Users/gopora/MyStuff/Dev/Workspaces/Sandbox/TroubledLife\"\n",
    "DATASETS_DIR = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "TF_LOG_DIR = os.path.join(PROJECT_ROOT_DIR, \"tf_logs\")\n",
    "MODEL_CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT_DIR, \"model_checkpoints\")\n",
    "TRAINING_SET_DATA_FILE = \"troubled_life_policy_train_data.csv\"\n",
    "TEST_SET_DATA_FILE = \"troubled_life_policy_test_data.csv\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"{}/run-{}/\".format(TF_LOG_DIR, now)\n",
    "\n",
    "\n",
    "import data_preparation as dp\n",
    "\n",
    "runtime = 5\n",
    "\n",
    "#dp.generate_troubled_life_policy_data(no_of_policies=10000, runtime=5, file_path=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "\n",
    "#dp.generate_troubled_life_policy_data(no_of_policies=2000, runtime=5, file_path=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_train = \\\n",
    "    dp.load_troubled_life_policy_data(file_path=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_test = \\\n",
    "    dp.load_troubled_life_policy_data(file_path=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_length_train, max_policy_history_length_train = \\\n",
    "    dp.get_policy_history_lengths(policy_histories=policy_histories_train)\n",
    "\n",
    "policy_histories_length_test, max_policy_history_length_test = \\\n",
    "    dp.get_policy_history_lengths(policy_histories=policy_histories_test)\n",
    "\n",
    "max_length_policy_history = max(max_policy_history_length_train, max_policy_history_length_test)\n",
    "\n",
    "\n",
    "\n",
    "# Pad the histories up to maximum length of both, train and test set\n",
    "\n",
    "# policy_histories_train = \\\n",
    "#     dp.pad_troubled_life_policy_histories(policy_histories=policy_histories_train,\n",
    "#                                           policy_histories_lengths=policy_histories_length_train,\n",
    "#                                           max_policy_history_length=max_policy_history_length)\n",
    "# \n",
    "# policy_histories_test = \\\n",
    "#     dp.pad_troubled_life_policy_histories(policy_histories=policy_histories_test,\n",
    "#                                           policy_histories_lengths=policy_histories_length_test,\n",
    "#                                           max_policy_history_length=max_policy_history_length)\n",
    "# \n",
    "# # # Save padded data, since always generating and padding takes too long\n",
    "# policy_histories_train.to_csv(path_or_buf=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "# policy_histories_test.to_csv(path_or_buf=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "# Extract features and labels from dataset as numpy.ndarray(s)\n",
    "binary_classification = True\n",
    "\n",
    "train_labels, train_features, train_seq_lengths =\\\n",
    "    dp.prepare_labels_features_lengths(policy_histories=policy_histories_train,\n",
    "                                       policy_histories_lengths=policy_histories_length_train, \n",
    "                                       max_policy_history_length=max_length_policy_history,\n",
    "                                       binary_classification=binary_classification)\n",
    "test_labels, test_features, test_seq_lengths =\\\n",
    "    dp.prepare_labels_features_lengths(policy_histories=policy_histories_test,\n",
    "                                       policy_histories_lengths=policy_histories_length_test, \n",
    "                                       max_policy_history_length=max_length_policy_history,\n",
    "                                       binary_classification=binary_classification)\n",
    "\n",
    "train_data = dp.TrainDataSet(train_labels=train_labels, train_features=train_features, train_seq_lengths=train_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(size_batch, Gz, Dg, init_z):\n",
    "    # discriminator should ideally fall for the fakes, so the discriminator's logits should flag 0\n",
    "    zeros = tf.zeros(size_batch, tf.int32)\n",
    "\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=zeros, logits=Dg)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(Dg, zeros, 1), tf.float32))\n",
    "    \n",
    "    interest = init_z[:, 1]\n",
    "    premium = init_z[:, 0]\n",
    "    \n",
    "    k1 = premium\n",
    "    k1v = k1 * (1 + interest)\n",
    "    k2 = premium + k1v\n",
    "    k2v = k2 * (1 + interest)\n",
    "    k3 = premium + k2v\n",
    "    k3v = k3 * (1 + interest)\n",
    "    k4 = premium + k3v\n",
    "    k4v = k4 * (1 + interest)\n",
    "    k5 = premium + k4v\n",
    "    k5v = k5 * (1 + interest)\n",
    "    \n",
    "    loss2 = tf.reduce_mean(tf.abs(Gz[:, 0, 0] - premium) + tf.abs(Gz[:, 0, 1]) +\n",
    "                           tf.abs(Gz[:, 1, 0] - premium) + tf.abs(Gz[:, 1, 1] - k1) +\n",
    "                           tf.abs(Gz[:, 2, 0] - premium) + tf.abs(Gz[:, 2, 1] - k1v) +\n",
    "                           tf.abs(Gz[:, 3, 0] - premium) + tf.abs(Gz[:, 3, 1] - k2) +\n",
    "                           tf.abs(Gz[:, 4, 0] - premium) + tf.abs(Gz[:, 4, 1] - k2v) +\n",
    "                           tf.abs(Gz[:, 5, 0] - premium) + tf.abs(Gz[:, 5, 1] - k3) +\n",
    "                           tf.abs(Gz[:, 6, 0] - premium) + tf.abs(Gz[:, 6, 1] - k3v) +\n",
    "                           tf.abs(Gz[:, 7, 0] - premium) + tf.abs(Gz[:, 7, 1] - k4) +\n",
    "                           tf.abs(Gz[:, 8, 0] - premium) + tf.abs(Gz[:, 8, 1] - k4v) +\n",
    "                           tf.abs(Gz[:, 9, 0] - premium) + tf.abs(Gz[:, 9, 1] - k5) +\n",
    "                           tf.abs(Gz[:, 10, 0] - premium) + tf.abs(Gz[:, 10, 1] - k5v)\n",
    "                           )\n",
    "\n",
    "    return loss2, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gru_g' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0d5234d4176e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m Gz = gan.generator(Z=Z, n_outputs=n_outputs_g, n_layers=n_layers_g, n_neurons=n_neurons_g, \n\u001b[0;32m---> 41\u001b[0;31m                    seq_length=seq_length_z, gru=gru_g, leaky=leaky_g, input_keep_prob=input_keep_prob_g)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m Dx, y_pred_x = gan.discriminator(X=X, seq_length=seq_length_x, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'gru_g' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gan as gan\n",
    "\n",
    "\n",
    "size_batch = 200\n",
    "\n",
    "# Generator hyperparameters\n",
    "gru_g=False\n",
    "leaky_g=False\n",
    "n_inputs_g = 2\n",
    "n_layers_g = 1\n",
    "n_neurons_g = 20\n",
    "n_outputs_g = 2\n",
    "input_keep_prob_g = 1.0\n",
    "learning_rate_g = 1e-2\n",
    "beta1_g=0.99\n",
    "beta2_g=0.999\n",
    "epsilon_g=1e-08\n",
    "clip_g=False\n",
    "threshold_g = 1.0\n",
    "\n",
    "# Discriminator hyperparameters\n",
    "learning_rate_d = 0.0001\n",
    "n_inputs_d = 2\n",
    "n_layers_d = 3\n",
    "n_neurons_d = 200\n",
    "n_outputs_d = 2 if binary_classification else max_length_policy_history\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "Z = tf.placeholder(tf.float32, [None, max_length_policy_history, n_inputs_g], name='Z')\n",
    "init_z = tf.placeholder(tf.float32, [None, n_inputs_g], name='init_z')\n",
    "seq_length_z = tf.placeholder(tf.int32, [None], name='seq_length_z')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_length_policy_history, n_inputs_d], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "seq_length_x = tf.placeholder(tf.int32, [None], name=\"seq_length_x\")\n",
    "\n",
    "Gz = gan.generator(Z=Z, n_outputs=n_outputs_g, n_layers=n_layers_g, n_neurons=n_neurons_g, \n",
    "                   seq_length=seq_length_z, gru=gru_g, leaky=leaky_g, input_keep_prob=input_keep_prob_g)\n",
    "\n",
    "Dx, y_pred_x = gan.discriminator(X=X, seq_length=seq_length_x, \n",
    "                                 n_layers=n_layers_d, n_neurons=n_neurons_d, n_outputs=n_outputs_d, leaky=True)\n",
    "Dg, y_pred_g = gan.discriminator(X=Gz, seq_length=seq_length_z,\n",
    "                                 n_layers=n_layers_d, n_neurons=n_neurons_d, n_outputs=n_outputs_d, leaky=True, reuse=True)\n",
    "\n",
    "loss_g, accuracy_g = generator_loss(size_batch=size_batch, Gz=Gz, Dg=Dg, init_z=init_z)\n",
    "loss_real_d, accuracy_real_d = gan.discriminator_loss_real(Dx=Dx, y=y)\n",
    "loss_fake_d, accuracy_fake_d = gan.discriminator_loss_fake(size_batch=size_batch, Dg=Dg)\n",
    "\n",
    "g_trainer = gan.generator_trainer(learning_rate=learning_rate_g, beta1=beta1_g, beta2=beta2_g, epsilon=epsilon_g, \n",
    "                                  clip=clip_g, threshold=threshold_g, loss=loss_g)\n",
    "d_trainer = gan.discriminator_trainer_real(learning_rate_d, loss_real_d + loss_fake_d)\n",
    "\n",
    "tf.summary.scalar('Generator_loss', loss_g)\n",
    "#tf.summary.scalar('Discriminator_loss_real', loss_real_d)\n",
    "#tf.summary.scalar('Discriminator_loss_fake', loss_fake_d)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "g_saver = tf.train.Saver(var_list=[var for var in tvars if \"t_generator\" in var.name])\n",
    "d_saver = tf.train.Saver(var_list=[var for var in tvars if \"t_discriminator\" in var.name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': (lambda x: \"%.2f\" % x)})\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "# Pre-train discriminator\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(train_data.num_examples // size_batch):\n",
    "        Z_batch, seq_length_z_batch, _ = dp.generate_Z_batch(\n",
    "            size_batch=size_batch, max_length_policy_history=max_length_policy_history, n_inputs = n_inputs_g, runtime=runtime)\n",
    " \n",
    "        y_batch, X_batch, seq_length_x_batch = train_data.next_batch(size_batch)\n",
    "\n",
    "        _, lossRealD, lossFakeD, accRealD, accFakeD = \\\n",
    "            sess.run([d_trainer, loss_real_d, loss_fake_d, accuracy_real_d, accuracy_fake_d],\n",
    "                     {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n",
    "                      Z: Z_batch, seq_length_z: seq_length_z_batch})\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"lossRealD:\", lossRealD, \"accRealD:\", accRealD,\n",
    "          \"lossFakeD:\", lossFakeD, \"accFakeD:\", accFakeD)\n",
    "\n",
    "d_saver.save(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"discriminator.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 lossG: 9240.36\n",
      "[1250.00 0.03]\n",
      "[[707.26 250.20]\n",
      " [715.16 1817.54]\n",
      " [549.20 2382.25]\n",
      " [693.70 2718.96]\n",
      " [673.04 3061.48]\n",
      " [906.20 3731.88]\n",
      " [967.64 4277.74]\n",
      " [1229.20 5113.80]\n",
      " [1338.84 5869.85]\n",
      " [1660.36 6960.55]\n",
      " [1841.75 8026.02]\n",
      " [0.13 0.28]\n",
      " [0.13 0.28]\n",
      " [0.13 0.28]\n",
      " [0.13 0.28]]\n",
      "Epoch: 1 lossG: 7140.05\n",
      "[1250.00 0.03]\n",
      "[[1327.73 -75.97]\n",
      " [813.44 1825.00]\n",
      " [559.21 2008.93]\n",
      " [889.22 2663.53]\n",
      " [774.44 2860.46]\n",
      " [1140.17 3681.51]\n",
      " [1106.12 4017.47]\n",
      " [1493.75 4939.65]\n",
      " [1499.71 5430.55]\n",
      " [1964.08 6559.65]\n",
      " [2003.41 7249.20]\n",
      " [0.38 0.27]\n",
      " [0.38 0.27]\n",
      " [0.38 0.27]\n",
      " [0.38 0.27]]\n",
      "Epoch: 2 lossG: 6283.93\n",
      "[1250.00 0.05]\n",
      "[[1399.29 25.45]\n",
      " [871.05 1865.88]\n",
      " [398.85 1878.41]\n",
      " [919.68 2831.36]\n",
      " [557.86 2804.06]\n",
      " [1088.77 3890.08]\n",
      " [817.43 3990.43]\n",
      " [1356.84 5139.58]\n",
      " [1113.48 5380.03]\n",
      " [1726.46 6713.05]\n",
      " [1480.61 7121.16]\n",
      " [0.64 0.24]\n",
      " [0.64 0.24]\n",
      " [0.64 0.24]\n",
      " [0.64 0.24]]\n",
      "Epoch: 3 lossG: 5260.43\n",
      "[1250.00 0.04]\n",
      "[[1154.62 15.45]\n",
      " [965.91 1627.56]\n",
      " [398.24 1599.77]\n",
      " [1034.02 2735.85]\n",
      " [554.90 2738.25]\n",
      " [1145.71 3940.52]\n",
      " [817.69 4017.43]\n",
      " [1381.86 5240.17]\n",
      " [1119.38 5476.59]\n",
      " [1736.66 6850.59]\n",
      " [1492.28 7290.48]\n",
      " [0.92 0.13]\n",
      " [0.92 0.13]\n",
      " [0.92 0.13]\n",
      " [0.92 0.13]]\n",
      "Epoch: 4 lossG: 3485.54\n",
      "[1250.00 0.04]\n",
      "[[1316.96 -31.85]\n",
      " [1160.30 1274.05]\n",
      " [735.51 1149.60]\n",
      " [1322.95 2465.09]\n",
      " [786.87 2513.98]\n",
      " [1318.01 3875.84]\n",
      " [981.97 3980.56]\n",
      " [1390.80 5191.61]\n",
      " [1272.99 5407.77]\n",
      " [1602.07 6594.85]\n",
      " [1622.96 7021.73]\n",
      " [1.26 -0.02]\n",
      " [1.26 -0.02]\n",
      " [1.26 -0.02]\n",
      " [1.26 -0.02]]\n",
      "Epoch: 5 lossG: 2440.59\n",
      "[1250.00 0.05]\n",
      "[[1293.20 65.99]\n",
      " [1324.59 1225.95]\n",
      " [1184.22 1183.24]\n",
      " [1431.56 2489.36]\n",
      " [1164.21 2618.22]\n",
      " [1243.75 4026.43]\n",
      " [1157.63 4222.96]\n",
      " [953.29 5334.80]\n",
      " [1174.50 5548.02]\n",
      " [1104.95 6471.51]\n",
      " [1357.40 6853.68]\n",
      " [1.52 -0.08]\n",
      " [1.52 -0.08]\n",
      " [1.52 -0.08]\n",
      " [1.52 -0.08]]\n",
      "Epoch: 6 lossG: 1865.74\n",
      "[1250.00 0.04]\n",
      "[[1119.52 -23.53]\n",
      " [1213.85 1311.09]\n",
      " [1293.61 1395.05]\n",
      " [1268.12 2500.79]\n",
      " [1326.60 2639.85]\n",
      " [1218.43 3923.36]\n",
      " [1345.66 4140.25]\n",
      " [1084.47 5307.85]\n",
      " [1345.52 5585.21]\n",
      " [1153.73 6663.77]\n",
      " [1514.97 7085.31]\n",
      " [1.64 -0.07]\n",
      " [1.64 -0.07]\n",
      " [1.64 -0.07]\n",
      " [1.64 -0.07]]\n",
      "Epoch: 7 lossG: 1375.98\n",
      "[1250.00 0.03]\n",
      "[[1203.11 -41.21]\n",
      " [1257.35 1358.78]\n",
      " [1139.18 1481.91]\n",
      " [1246.41 2583.14]\n",
      " [1162.81 2627.80]\n",
      " [1265.45 3923.64]\n",
      " [1190.15 4031.96]\n",
      " [1235.06 5327.82]\n",
      " [1185.08 5514.84]\n",
      " [1174.95 6780.65]\n",
      " [1269.19 7019.74]\n",
      " [1.75 -0.07]\n",
      " [1.75 -0.07]\n",
      " [1.75 -0.07]\n",
      " [1.75 -0.07]]\n",
      "Epoch: 8 lossG: 1239.73\n",
      "[1250.00 0.05]\n",
      "[[1367.63 8.51]\n",
      " [1255.97 1230.75]\n",
      " [1243.24 1478.43]\n",
      " [1235.92 2563.02]\n",
      " [1253.28 2680.41]\n",
      " [1264.92 3907.99]\n",
      " [1228.44 4074.24]\n",
      " [1244.85 5304.01]\n",
      " [1153.59 5529.78]\n",
      " [1182.98 6721.00]\n",
      " [1115.32 6993.46]\n",
      " [1.86 -0.11]\n",
      " [1.86 -0.11]\n",
      " [1.86 -0.11]\n",
      " [1.86 -0.11]]\n",
      "Epoch: 9 lossG: 822.542\n",
      "[1250.00 0.05]\n",
      "[[1211.44 -8.87]\n",
      " [1250.86 1186.45]\n",
      " [1247.64 1292.56]\n",
      " [1234.76 2561.08]\n",
      " [1262.82 2610.48]\n",
      " [1235.26 3929.39]\n",
      " [1246.00 4050.49]\n",
      " [1222.14 5328.59]\n",
      " [1211.10 5535.45]\n",
      " [1195.00 6753.47]\n",
      " [1241.09 7030.99]\n",
      " [1.93 -0.14]\n",
      " [1.93 -0.14]\n",
      " [1.93 -0.14]\n",
      " [1.93 -0.14]]\n",
      "Epoch: 10 lossG: 847.915\n",
      "[1250.00 0.03]\n",
      "[[1270.92 62.06]\n",
      " [1251.71 1264.68]\n",
      " [1246.86 1310.07]\n",
      " [1240.84 2583.84]\n",
      " [1235.22 2669.46]\n",
      " [1225.41 3934.35]\n",
      " [1220.33 4093.18]\n",
      " [1220.87 5299.02]\n",
      " [1214.50 5540.29]\n",
      " [1226.15 6695.48]\n",
      " [1235.46 7019.27]\n",
      " [1.98 -0.12]\n",
      " [1.98 -0.12]\n",
      " [1.98 -0.12]\n",
      " [1.98 -0.12]]\n",
      "Epoch: 11 lossG: 933.533\n",
      "[1250.00 0.04]\n",
      "[[1219.44 -43.07]\n",
      " [1243.65 1262.51]\n",
      " [1236.42 1319.16]\n",
      " [1249.47 2569.05]\n",
      " [1235.98 2691.24]\n",
      " [1233.22 3937.00]\n",
      " [1225.40 4131.49]\n",
      " [1221.47 5326.21]\n",
      " [1217.15 5600.75]\n",
      " [1211.37 6744.14]\n",
      " [1211.07 7102.07]\n",
      " [2.00 -0.13]\n",
      " [2.00 -0.13]\n",
      " [2.00 -0.13]\n",
      " [2.00 -0.13]]\n",
      "Epoch: 12 lossG: 726.585\n",
      "[1250.00 0.03]\n",
      "[[1252.71 30.47]\n",
      " [1258.39 1248.47]\n",
      " [1232.84 1295.24]\n",
      " [1258.53 2543.26]\n",
      " [1235.61 2645.94]\n",
      " [1258.31 3914.77]\n",
      " [1238.57 4077.77]\n",
      " [1263.51 5329.55]\n",
      " [1246.69 5562.78]\n",
      " [1273.57 6800.63]\n",
      " [1260.24 7108.39]\n",
      " [2.03 -0.13]\n",
      " [2.03 -0.13]\n",
      " [2.03 -0.13]\n",
      " [2.03 -0.13]]\n",
      "Epoch: 13 lossG: 982.136\n",
      "[1250.00 0.05]\n",
      "[[1243.90 1.56]\n",
      " [1250.22 1244.31]\n",
      " [1247.58 1301.99]\n",
      " [1238.46 2534.02]\n",
      " [1238.40 2622.49]\n",
      " [1233.92 3876.68]\n",
      " [1222.71 4014.80]\n",
      " [1225.48 5250.33]\n",
      " [1202.56 5446.73]\n",
      " [1212.28 6657.41]\n",
      " [1177.85 6915.02]\n",
      " [2.04 -0.13]\n",
      " [2.04 -0.13]\n",
      " [2.04 -0.13]\n",
      " [2.04 -0.13]]\n",
      "Epoch: 14 lossG: 1148.91\n",
      "[1250.00 0.03]\n",
      "[[1234.53 -19.71]\n",
      " [1234.97 1245.26]\n",
      " [1239.37 1302.07]\n",
      " [1227.44 2533.67]\n",
      " [1235.94 2620.53]\n",
      " [1217.86 3864.15]\n",
      " [1218.69 3999.11]\n",
      " [1206.87 5222.45]\n",
      " [1198.23 5413.01]\n",
      " [1190.64 6609.19]\n",
      " [1173.03 6858.00]\n",
      " [2.06 -0.12]\n",
      " [2.06 -0.12]\n",
      " [2.06 -0.12]\n",
      " [2.06 -0.12]]\n",
      "Epoch: 15 lossG: 2067.95\n",
      "[1250.00 0.04]\n",
      "[[1229.68 -7.38]\n",
      " [1249.33 1238.25]\n",
      " [1218.27 1263.36]\n",
      " [1238.32 2506.79]\n",
      " [1219.07 2567.65]\n",
      " [1219.29 3804.44]\n",
      " [1200.96 3908.31]\n",
      " [1201.44 5113.12]\n",
      " [1182.13 5266.99]\n",
      " [1178.82 6433.43]\n",
      " [1159.69 6639.35]\n",
      " [2.07 -0.12]\n",
      " [2.07 -0.12]\n",
      " [2.07 -0.12]\n",
      " [2.07 -0.12]]\n",
      "Epoch: 16 lossG: 1603.11\n",
      "[1250.00 0.05]\n",
      "[[1263.25 -4.59]\n",
      " [1230.10 1236.74]\n",
      " [1240.44 1290.84]\n",
      " [1221.60 2498.18]\n",
      " [1235.35 2599.17]\n",
      " [1210.86 3814.17]\n",
      " [1218.31 3956.52]\n",
      " [1198.40 5151.54]\n",
      " [1199.61 5342.36]\n",
      " [1180.54 6511.07]\n",
      " [1176.61 6752.49]\n",
      " [2.09 -0.10]\n",
      " [2.09 -0.10]\n",
      " [2.09 -0.10]\n",
      " [2.09 -0.10]]\n",
      "Epoch: 17 lossG: 665.477\n",
      "[1250.00 0.05]\n",
      "[[1258.20 8.32]\n",
      " [1246.55 1252.66]\n",
      " [1249.89 1299.28]\n",
      " [1245.41 2539.58]\n",
      " [1252.34 2638.16]\n",
      " [1245.10 3894.83]\n",
      " [1247.60 4034.92]\n",
      " [1244.55 5282.30]\n",
      " [1243.80 5472.19]\n",
      " [1240.79 6706.30]\n",
      " [1238.14 6948.72]\n",
      " [2.11 -0.08]\n",
      " [2.11 -0.08]\n",
      " [2.11 -0.08]\n",
      " [2.11 -0.08]]\n",
      "Epoch: 18 lossG: 738.465\n",
      "[1250.00 0.05]\n",
      "[[1253.52 0.24]\n",
      " [1244.52 1248.66]\n",
      " [1242.82 1289.54]\n",
      " [1241.82 2531.72]\n",
      " [1239.76 2628.41]\n",
      " [1237.58 3887.67]\n",
      " [1232.85 4028.89]\n",
      " [1235.71 5283.49]\n",
      " [1230.06 5477.98]\n",
      " [1233.29 6726.91]\n",
      " [1228.29 6977.92]\n",
      " [2.12 -0.08]\n",
      " [2.12 -0.08]\n",
      " [2.12 -0.08]\n",
      " [2.12 -0.08]]\n",
      "Epoch: 19 lossG: 571.347\n",
      "[1250.00 0.03]\n",
      "[[1251.57 -7.03]\n",
      " [1246.92 1246.83]\n",
      " [1241.99 1297.68]\n",
      " [1246.85 2543.81]\n",
      " [1243.03 2645.68]\n",
      " [1242.06 3905.48]\n",
      " [1239.06 4055.96]\n",
      " [1243.62 5315.54]\n",
      " [1242.73 5523.38]\n",
      " [1247.12 6784.12]\n",
      " [1250.17 7053.13]\n",
      " [2.13 -0.07]\n",
      " [2.13 -0.07]\n",
      " [2.13 -0.07]\n",
      " [2.13 -0.07]]\n",
      "Epoch: 20 lossG: 704.288\n",
      "[1250.00 0.04]\n",
      "[[1249.28 4.44]\n",
      " [1253.69 1255.39]\n",
      " [1253.94 1309.78]\n",
      " [1255.54 2560.64]\n",
      " [1259.24 2661.35]\n",
      " [1259.14 3924.29]\n",
      " [1259.95 4073.31]\n",
      " [1266.29 5334.37]\n",
      " [1266.36 5540.52]\n",
      " [1273.21 6798.80]\n",
      " [1274.39 7065.70]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]]\n",
      "Epoch: 21 lossG: 586.614\n",
      "[1250.00 0.04]\n",
      "[[1251.47 3.38]\n",
      " [1255.16 1251.78]\n",
      " [1249.06 1306.38]\n",
      " [1255.56 2555.91]\n",
      " [1251.34 2661.11]\n",
      " [1253.53 3919.18]\n",
      " [1246.10 4076.79]\n",
      " [1251.83 5328.12]\n",
      " [1243.64 5547.17]\n",
      " [1246.62 6787.78]\n",
      " [1239.60 7071.95]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]\n",
      " [2.14 -0.06]]\n",
      "Epoch: 22 lossG: 648.412\n",
      "[1250.00 0.05]\n",
      "[[1255.67 -0.65]\n",
      " [1244.37 1249.21]\n",
      " [1255.26 1286.48]\n",
      " [1246.94 2532.70]\n",
      " [1258.58 2622.75]\n",
      " [1249.95 3876.16]\n",
      " [1260.70 4016.94]\n",
      " [1254.66 5263.01]\n",
      " [1268.02 5463.62]\n",
      " [1258.61 6701.12]\n",
      " [1276.62 6965.14]\n",
      " [2.15 -0.06]\n",
      " [2.15 -0.06]\n",
      " [2.15 -0.06]\n",
      " [2.15 -0.06]]\n",
      "Epoch: 23 lossG: 592.439\n",
      "[1250.00 0.03]\n",
      "[[1251.22 -1.17]\n",
      " [1249.03 1252.78]\n",
      " [1249.65 1303.83]\n",
      " [1252.56 2557.32]\n",
      " [1246.73 2661.34]\n",
      " [1256.48 3924.19]\n",
      " [1244.28 4082.73]\n",
      " [1258.23 5336.55]\n",
      " [1243.69 5559.48]\n",
      " [1256.79 6800.69]\n",
      " [1241.65 7091.78]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]]\n",
      "Epoch: 24 lossG: 620.364\n",
      "[1250.00 0.04]\n",
      "[[1251.75 0.39]\n",
      " [1252.49 1248.88]\n",
      " [1250.04 1294.26]\n",
      " [1252.66 2549.15]\n",
      " [1250.12 2640.96]\n",
      " [1250.64 3892.46]\n",
      " [1246.44 4034.86]\n",
      " [1247.98 5267.46]\n",
      " [1245.25 5469.41]\n",
      " [1242.11 6679.27]\n",
      " [1242.74 6943.79]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]\n",
      " [2.15 -0.05]]\n",
      "Epoch: 25 lossG: 566.737\n",
      "[1250.00 0.03]\n",
      "[[1252.34 1.78]\n",
      " [1252.92 1253.43]\n",
      " [1250.24 1313.51]\n",
      " [1248.12 2548.00]\n",
      " [1249.55 2655.70]\n",
      " [1245.49 3893.12]\n",
      " [1248.11 4052.55]\n",
      " [1243.59 5278.84]\n",
      " [1250.78 5498.99]\n",
      " [1240.23 6712.28]\n",
      " [1253.97 6996.53]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]]\n",
      "Epoch: 26 lossG: 644.537\n",
      "[1250.00 0.03]\n",
      "[[1252.98 0.63]\n",
      " [1258.21 1248.48]\n",
      " [1247.16 1298.54]\n",
      " [1256.15 2558.04]\n",
      " [1244.81 2648.85]\n",
      " [1251.95 3908.98]\n",
      " [1238.86 4049.93]\n",
      " [1248.69 5297.20]\n",
      " [1236.35 5497.50]\n",
      " [1243.37 6729.25]\n",
      " [1233.56 6992.15]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]]\n",
      "Epoch: 27 lossG: 538.148\n",
      "[1250.00 0.04]\n",
      "[[1254.15 -3.52]\n",
      " [1248.12 1246.02]\n",
      " [1252.68 1304.89]\n",
      " [1250.88 2546.87]\n",
      " [1255.31 2653.02]\n",
      " [1247.55 3898.56]\n",
      " [1250.82 4054.31]\n",
      " [1245.49 5290.98]\n",
      " [1249.33 5505.16]\n",
      " [1240.51 6729.49]\n",
      " [1246.70 7005.38]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]\n",
      " [2.16 -0.05]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 lossG: 541.571\n",
      "[1250.00 0.04]\n",
      "[[1250.05 2.38]\n",
      " [1247.69 1247.63]\n",
      " [1250.71 1301.52]\n",
      " [1239.54 2543.71]\n",
      " [1250.02 2650.49]\n",
      " [1243.30 3904.48]\n",
      " [1249.61 4059.41]\n",
      " [1246.48 5310.85]\n",
      " [1251.97 5523.48]\n",
      " [1247.42 6770.06]\n",
      " [1253.79 7043.79]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]]\n",
      "Epoch: 29 lossG: 604.071\n",
      "[1250.00 0.04]\n",
      "[[1251.60 -4.98]\n",
      " [1250.11 1249.41]\n",
      " [1241.67 1299.81]\n",
      " [1247.97 2550.76]\n",
      " [1241.75 2650.47]\n",
      " [1242.12 3904.28]\n",
      " [1236.42 4054.44]\n",
      " [1240.28 5304.05]\n",
      " [1236.69 5513.87]\n",
      " [1238.03 6758.42]\n",
      " [1238.34 7031.52]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]]\n",
      "Epoch: 30 lossG: 659.477\n",
      "[1250.00 0.03]\n",
      "[[1253.76 1.28]\n",
      " [1257.42 1251.59]\n",
      " [1249.25 1307.42]\n",
      " [1248.83 2555.30]\n",
      " [1245.07 2662.10]\n",
      " [1244.41 3916.85]\n",
      " [1238.73 4075.59]\n",
      " [1241.84 5328.62]\n",
      " [1236.65 5548.99]\n",
      " [1238.18 6799.11]\n",
      " [1235.18 7084.94]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]]\n",
      "Epoch: 31 lossG: 815.645\n",
      "[1250.00 0.03]\n",
      "[[1251.15 3.57]\n",
      " [1246.88 1248.49]\n",
      " [1243.73 1299.98]\n",
      " [1246.55 2557.24]\n",
      " [1238.02 2666.33]\n",
      " [1245.15 3938.31]\n",
      " [1231.57 4104.23]\n",
      " [1242.10 5378.11]\n",
      " [1226.48 5611.72]\n",
      " [1235.86 6884.50]\n",
      " [1219.68 7190.20]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]\n",
      " [2.17 -0.04]]\n",
      "Epoch: 32 lossG: 679.96\n",
      "[1250.00 0.04]\n",
      "[[1268.54 1.02]\n",
      " [1263.65 1259.09]\n",
      " [1264.41 1308.85]\n",
      " [1266.62 2568.43]\n",
      " [1263.39 2668.65]\n",
      " [1263.53 3932.85]\n",
      " [1257.86 4086.34]\n",
      " [1258.75 5342.94]\n",
      " [1253.10 5559.59]\n",
      " [1249.67 6804.01]\n",
      " [1245.61 7087.39]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]]\n",
      "Epoch: 33 lossG: 706.737\n",
      "[1250.00 0.05]\n",
      "[[1245.28 5.49]\n",
      " [1241.34 1231.58]\n",
      " [1239.16 1289.80]\n",
      " [1238.02 2523.15]\n",
      " [1233.33 2643.41]\n",
      " [1237.91 3884.56]\n",
      " [1229.97 4059.90]\n",
      " [1237.31 5297.36]\n",
      " [1229.59 5538.27]\n",
      " [1235.44 6770.83]\n",
      " [1229.46 7081.31]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]\n",
      " [2.18 -0.04]]\n",
      "Epoch: 34 lossG: 652.557\n",
      "[1250.00 0.03]\n",
      "[[1245.35 -2.63]\n",
      " [1245.27 1242.10]\n",
      " [1252.88 1292.40]\n",
      " [1240.85 2537.41]\n",
      " [1255.14 2643.94]\n",
      " [1248.00 3895.01]\n",
      " [1259.63 4055.38]\n",
      " [1259.45 5307.57]\n",
      " [1271.18 5531.87]\n",
      " [1273.06 6787.03]\n",
      " [1286.51 7079.39]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]]\n",
      "Epoch: 35 lossG: 567.221\n",
      "[1250.00 0.04]\n",
      "[[1257.53 -2.34]\n",
      " [1240.59 1244.51]\n",
      " [1249.32 1302.58]\n",
      " [1244.56 2543.44]\n",
      " [1256.88 2652.52]\n",
      " [1243.55 3895.14]\n",
      " [1253.67 4057.43]\n",
      " [1244.19 5296.66]\n",
      " [1252.96 5521.43]\n",
      " [1241.18 6753.10]\n",
      " [1250.21 7044.00]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]\n",
      " [2.19 -0.04]]\n",
      "Epoch: 36 lossG: 665.56\n",
      "[1250.00 0.05]\n",
      "[[1242.37 2.74]\n",
      " [1254.22 1238.36]\n",
      " [1251.58 1293.59]\n",
      " [1254.80 2537.06]\n",
      " [1252.83 2637.77]\n",
      " [1261.85 3895.07]\n",
      " [1259.13 4043.01]\n",
      " [1269.73 5303.52]\n",
      " [1269.82 5508.86]\n",
      " [1277.95 6772.83]\n",
      " [1282.41 7039.17]\n",
      " [2.20 -0.04]\n",
      " [2.20 -0.04]\n",
      " [2.20 -0.04]\n",
      " [2.20 -0.04]]\n",
      "Epoch: 37 lossG: 514.058\n",
      "[1250.00 0.04]\n",
      "[[1252.19 1.20]\n",
      " [1254.66 1250.56]\n",
      " [1251.24 1304.98]\n",
      " [1250.64 2548.08]\n",
      " [1249.49 2655.99]\n",
      " [1249.53 3909.62]\n",
      " [1249.15 4067.95]\n",
      " [1248.94 5322.75]\n",
      " [1252.46 5541.52]\n",
      " [1247.38 6796.64]\n",
      " [1256.46 7079.59]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]]\n",
      "Epoch: 38 lossG: 589.13\n",
      "[1250.00 0.03]\n",
      "[[1249.35 -1.64]\n",
      " [1247.25 1249.53]\n",
      " [1247.60 1295.54]\n",
      " [1243.62 2543.22]\n",
      " [1249.70 2638.76]\n",
      " [1245.45 3885.86]\n",
      " [1249.93 4029.84]\n",
      " [1249.98 5272.33]\n",
      " [1255.11 5474.09]\n",
      " [1254.21 6711.29]\n",
      " [1261.60 6974.12]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]\n",
      " [2.20 -0.03]]\n",
      "Epoch: 39 lossG: 845.661\n",
      "[1250.00 0.04]\n",
      "[[1246.79 -4.17]\n",
      " [1251.27 1253.99]\n",
      " [1244.95 1304.81]\n",
      " [1247.02 2565.44]\n",
      " [1237.82 2675.75]\n",
      " [1237.83 3934.58]\n",
      " [1223.75 4101.38]\n",
      " [1226.07 5348.57]\n",
      " [1208.91 5581.94]\n",
      " [1208.67 6811.65]\n",
      " [1195.90 7114.41]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]]\n",
      "Epoch: 40 lossG: 704.292\n",
      "[1250.00 0.03]\n",
      "[[1241.64 1.72]\n",
      " [1241.30 1247.28]\n",
      " [1248.54 1297.86]\n",
      " [1251.28 2536.73]\n",
      " [1253.65 2633.82]\n",
      " [1251.90 3867.59]\n",
      " [1251.59 4011.74]\n",
      " [1253.60 5234.09]\n",
      " [1251.74 5434.13]\n",
      " [1251.93 6641.03]\n",
      " [1270.07 6897.64]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]\n",
      " [2.21 -0.03]]\n",
      "Epoch: 41 lossG: 769.405\n",
      "[1250.00 0.05]\n",
      "[[1250.93 2.33]\n",
      " [1248.97 1248.22]\n",
      " [1249.24 1304.69]\n",
      " [1241.16 2548.02]\n",
      " [1242.81 2649.78]\n",
      " [1233.37 3890.90]\n",
      " [1227.47 4042.63]\n",
      " [1219.40 5267.71]\n",
      " [1207.66 5478.69]\n",
      " [1196.11 6678.22]\n",
      " [1224.74 6946.53]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]]\n",
      "Epoch: 42 lossG: 557.693\n",
      "[1250.00 0.03]\n",
      "[[1248.95 2.64]\n",
      " [1243.71 1250.39]\n",
      " [1249.03 1304.93]\n",
      " [1237.69 2544.51]\n",
      " [1251.39 2647.05]\n",
      " [1245.46 3901.66]\n",
      " [1253.66 4057.53]\n",
      " [1248.83 5314.18]\n",
      " [1254.35 5533.94]\n",
      " [1246.67 6787.19]\n",
      " [1250.70 7074.65]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]]\n",
      "Epoch: 43 lossG: 666.8\n",
      "[1250.00 0.03]\n",
      "[[1247.61 3.25]\n",
      " [1235.43 1239.49]\n",
      " [1246.23 1287.28]\n",
      " [1249.37 2533.87]\n",
      " [1254.28 2631.52]\n",
      " [1254.27 3885.60]\n",
      " [1260.86 4035.38]\n",
      " [1261.19 5289.60]\n",
      " [1270.61 5502.10]\n",
      " [1267.00 6754.76]\n",
      " [1280.59 7033.82]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]\n",
      " [2.22 -0.02]]\n",
      "Epoch: 44 lossG: 668.031\n",
      "[1250.00 0.03]\n",
      "[[1241.25 0.11]\n",
      " [1249.75 1245.75]\n",
      " [1251.19 1290.93]\n",
      " [1245.13 2533.80]\n",
      " [1240.79 2645.16]\n",
      " [1238.32 3897.67]\n",
      " [1234.96 4065.87]\n",
      " [1232.19 5316.70]\n",
      " [1231.76 5553.28]\n",
      " [1225.62 6802.25]\n",
      " [1229.25 7111.42]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]]\n",
      "Epoch: 45 lossG: 576.432\n",
      "[1250.00 0.04]\n",
      "[[1257.32 -1.13]\n",
      " [1249.92 1249.56]\n",
      " [1253.07 1301.93]\n",
      " [1244.54 2546.73]\n",
      " [1254.15 2650.60]\n",
      " [1245.64 3895.52]\n",
      " [1252.43 4047.29]\n",
      " [1247.82 5286.16]\n",
      " [1252.56 5495.99]\n",
      " [1247.81 6726.22]\n",
      " [1251.57 6997.18]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]\n",
      " [2.23 -0.02]]\n",
      "Epoch: 46 lossG: 690.778\n",
      "[1250.00 0.03]\n",
      "[[1256.16 0.21]\n",
      " [1249.14 1253.42]\n",
      " [1244.33 1296.89]\n",
      " [1253.28 2543.51]\n",
      " [1246.98 2637.83]\n",
      " [1253.88 3887.63]\n",
      " [1249.88 4030.11]\n",
      " [1260.03 5280.67]\n",
      " [1258.83 5481.75]\n",
      " [1268.20 6734.54]\n",
      " [1270.98 6997.77]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]]\n",
      "Epoch: 47 lossG: 644.692\n",
      "[1250.00 0.03]\n",
      "[[1253.13 1.16]\n",
      " [1250.90 1253.72]\n",
      " [1250.54 1301.12]\n",
      " [1252.28 2555.01]\n",
      " [1252.46 2652.52]\n",
      " [1255.70 3908.05]\n",
      " [1256.30 4048.81]\n",
      " [1259.66 5297.45]\n",
      " [1262.29 5491.84]\n",
      " [1262.59 6732.14]\n",
      " [1268.30 6982.90]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]]\n",
      "Epoch: 48 lossG: 526.063\n",
      "[1250.00 0.04]\n",
      "[[1253.52 -4.73]\n",
      " [1248.53 1249.48]\n",
      " [1249.36 1298.72]\n",
      " [1247.83 2544.77]\n",
      " [1248.72 2648.54]\n",
      " [1246.45 3899.67]\n",
      " [1248.47 4054.89]\n",
      " [1247.99 5306.16]\n",
      " [1252.06 5523.60]\n",
      " [1249.70 6774.94]\n",
      " [1256.94 7058.33]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]\n",
      " [2.24 -0.01]]\n",
      "Epoch: 49 lossG: 573.63\n",
      "[1250.00 0.04]\n",
      "[[1251.38 6.88]\n",
      " [1254.89 1252.36]\n",
      " [1251.34 1307.55]\n",
      " [1255.73 2556.42]\n",
      " [1256.23 2659.66]\n",
      " [1257.76 3911.41]\n",
      " [1259.51 4062.30]\n",
      " [1263.20 5313.82]\n",
      " [1266.74 5522.77]\n",
      " [1268.61 6773.54]\n",
      " [1275.08 7043.90]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]]\n",
      "Epoch: 50 lossG: 639.677\n",
      "[1250.00 0.03]\n",
      "[[1244.74 -4.83]\n",
      " [1252.27 1251.56]\n",
      " [1245.74 1294.31]\n",
      " [1245.99 2543.82]\n",
      " [1243.13 2641.49]\n",
      " [1242.01 3895.93]\n",
      " [1236.05 4045.84]\n",
      " [1237.31 5298.26]\n",
      " [1228.96 5511.15]\n",
      " [1228.63 6757.50]\n",
      " [1218.89 7036.95]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]]\n",
      "Epoch: 51 lossG: 798.427\n",
      "[1250.00 0.03]\n",
      "[[1254.84 5.25]\n",
      " [1250.48 1254.44]\n",
      " [1255.82 1300.41]\n",
      " [1261.06 2561.83]\n",
      " [1265.37 2658.02]\n",
      " [1268.59 3927.26]\n",
      " [1271.57 4073.16]\n",
      " [1278.28 5346.23]\n",
      " [1280.40 5552.67]\n",
      " [1286.46 6827.53]\n",
      " [1288.79 7098.17]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]\n",
      " [2.25 -0.01]]\n",
      "Epoch: 52 lossG: 610.885\n",
      "[1250.00 0.04]\n",
      "[[1247.20 -1.36]\n",
      " [1254.88 1250.58]\n",
      " [1248.69 1300.26]\n",
      " [1246.63 2542.22]\n",
      " [1244.60 2641.37]\n",
      " [1245.80 3888.27]\n",
      " [1241.46 4034.97]\n",
      " [1244.70 5279.86]\n",
      " [1239.80 5484.59]\n",
      " [1241.44 6725.34]\n",
      " [1237.06 6991.26]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]]\n",
      "Epoch: 53 lossG: 684.349\n",
      "[1250.00 0.05]\n",
      "[[1247.88 4.62]\n",
      " [1250.99 1249.26]\n",
      " [1239.81 1301.13]\n",
      " [1247.49 2541.45]\n",
      " [1239.16 2650.69]\n",
      " [1239.55 3889.62]\n",
      " [1233.96 4048.68]\n",
      " [1233.28 5283.77]\n",
      " [1231.17 5503.12]\n",
      " [1224.66 6731.95]\n",
      " [1227.39 7014.89]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]\n",
      " [2.26 -0.00]]\n",
      "Epoch: 54 lossG: 588.454\n",
      "[1250.00 0.05]\n",
      "[[1253.61 0.70]\n",
      " [1248.99 1255.75]\n",
      " [1245.88 1302.06]\n",
      " [1245.23 2555.37]\n",
      " [1242.97 2659.10]\n",
      " [1242.78 3910.92]\n",
      " [1239.26 4065.37]\n",
      " [1240.90 5312.54]\n",
      " [1237.84 5528.38]\n",
      " [1236.97 6768.81]\n",
      " [1235.59 7049.38]\n",
      " [2.26 0.00]\n",
      " [2.26 0.00]\n",
      " [2.26 0.00]\n",
      " [2.26 0.00]]\n"
     ]
    }
   ],
   "source": [
    "#d_saver.restore(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"discriminator.ckpt\"))\n",
    "#g_saver.restore(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"generator.ckpt\"))\n",
    "\n",
    "# Train generator and discriminator together\n",
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(train_data.num_examples // size_batch):\n",
    "        Z_batch, seq_length_z_batch, init_z_batch = dp.generate_Z_batch(\n",
    "            size_batch=size_batch, max_length_policy_history=max_length_policy_history, \n",
    "            n_inputs = n_inputs_g, runtime=runtime)\n",
    "        \n",
    "        # y_batch, X_batch, seq_length_x_batch = train_data.next_batch(size_batch)\n",
    "        # \n",
    "       #   # Train discriminator on both real and fake data\n",
    "       #  _, lossRealD, accRealD = \\\n",
    "       #      sess.run([d_trainer, loss_real_d, accuracy_real_d],\n",
    "       #               {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n",
    "       #                Z: Z_batch, seq_length_z: seq_length_z_batch})\n",
    "       # \n",
    "       #  Z_batch, seq_length_z_batch, init_z_batch = dp.generate_Z_batch(\n",
    "       #      size_batch=size_batch, max_length_policy_history=max_length_policy_history, \n",
    "       #      n_inputs = n_inputs_g, runtime=runtime)\n",
    "\n",
    "        # # Train generator    \n",
    "        # _, lossFakeD, accFakeD, lossG, accG, gData = \\\n",
    "        #     sess.run([g_trainer, loss_fake_d, accuracy_fake_d, loss_g, accuracy_g, Gz], \n",
    "        #              feed_dict={Z: Z_batch, seq_length_z: seq_length_z_batch, init_z: init_z_batch})\n",
    "\n",
    "        # Train generator    \n",
    "        _, lossG, gData = \\\n",
    "            sess.run([g_trainer, loss_g, Gz], \n",
    "                     feed_dict={Z: Z_batch, seq_length_z: seq_length_z_batch, init_z: init_z_batch})\n",
    "\n",
    "    # summary = sess.run(merged, feed_dict={X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n",
    "    #                       Z: Z_batch, seq_length_z: seq_length_z_batch, init_z: init_z_batch})\n",
    "    # writer.add_summary(summary, epoch)\n",
    "\n",
    "    summary = sess.run(merged, feed_dict={Z: Z_batch, seq_length_z: seq_length_z_batch, init_z: init_z_batch})\n",
    "    writer.add_summary(summary, epoch)\n",
    "\n",
    "    print(\"Epoch:\", epoch, \n",
    "          #\"lossRealD:\", lossRealD, \"accRealD:\", accRealD,\n",
    "          #\"lossFakeD:\", lossFakeD, \"accFakeD:\", accFakeD,\n",
    "          #\"lossG:\", lossG, \"accG:\", accG)\n",
    "          \"lossG:\", lossG)\n",
    "    \n",
    "    print(Z_batch[0, 0])\n",
    "    print(gData[0])\n",
    "    \n",
    "g_saver.save(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"generator3.ckpt\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/TroubledLife/model_checkpoints/generator.ckpt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_saver.save(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"generator.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 't_generator/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(22, 20) dtype=float32_ref>\n",
      "<tf.Variable 't_generator/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(20,) dtype=float32_ref>\n",
      "<tf.Variable 't_generator/dense/kernel:0' shape=(20, 2) dtype=float32_ref>\n",
      "<tf.Variable 't_generator/dense/bias:0' shape=(2,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for var in tvars:\n",
    "    if \"t_generator\" in var.name:\n",
    "        print(var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
