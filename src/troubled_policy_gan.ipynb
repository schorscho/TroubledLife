{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ROOT_DIR = \"/Users/gopora/MyStuff/Dev/Workspaces/Sandbox/TroubledLife\"\n",
    "DATASETS_DIR = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "TF_LOG_DIR = os.path.join(PROJECT_ROOT_DIR, \"tf_logs\")\n",
    "MODEL_CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT_DIR, \"model_checkpoints\")\n",
    "TRAINING_SET_DATA_FILE = \"troubled_life_policy_train_data.csv\"\n",
    "TEST_SET_DATA_FILE = \"troubled_life_policy_test_data.csv\"\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"{}/run-{}/\".format(TF_LOG_DIR, now)\n",
    "\n",
    "\n",
    "import data_preparation as dp\n",
    "\n",
    "#dp.generate_troubled_life_policy_data(no_of_policies=10000, runtime=5, file_path=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "\n",
    "#dp.generate_troubled_life_policy_data(no_of_policies=2000, runtime=5, file_path=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_train = \\\n",
    "    dp.load_troubled_life_policy_data(file_path=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_test = \\\n",
    "    dp.load_troubled_life_policy_data(file_path=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "policy_histories_length_train, max_policy_history_length_train = \\\n",
    "    dp.get_policy_history_lengths(policy_histories=policy_histories_train)\n",
    "\n",
    "policy_histories_length_test, max_policy_history_length_test = \\\n",
    "    dp.get_policy_history_lengths(policy_histories=policy_histories_test)\n",
    "\n",
    "max_length_policy_history = max(max_policy_history_length_train, max_policy_history_length_test)\n",
    "\n",
    "\n",
    "\n",
    "# Pad the histories up to maximum length of both, train and test set\n",
    "\n",
    "# policy_histories_train = \\\n",
    "#     dp.pad_troubled_life_policy_histories(policy_histories=policy_histories_train,\n",
    "#                                           policy_histories_lengths=policy_histories_length_train,\n",
    "#                                           max_policy_history_length=max_policy_history_length)\n",
    "# \n",
    "# policy_histories_test = \\\n",
    "#     dp.pad_troubled_life_policy_histories(policy_histories=policy_histories_test,\n",
    "#                                           policy_histories_lengths=policy_histories_length_test,\n",
    "#                                           max_policy_history_length=max_policy_history_length)\n",
    "# \n",
    "# # # Save padded data, since always generating and padding takes too long\n",
    "# policy_histories_train.to_csv(path_or_buf=os.path.join(DATASETS_DIR, TRAINING_SET_DATA_FILE))\n",
    "# policy_histories_test.to_csv(path_or_buf=os.path.join(DATASETS_DIR, TEST_SET_DATA_FILE))\n",
    "\n",
    "# Extract features and labels from dataset as numpy.ndarray(s)\n",
    "binary_classification = True\n",
    "\n",
    "train_labels, train_features, train_seq_lengths =\\\n",
    "    dp.prepare_labels_features_lengths(policy_histories=policy_histories_train,\n",
    "                                       policy_histories_lengths=policy_histories_length_train, \n",
    "                                       max_policy_history_length=max_length_policy_history,\n",
    "                                       binary_classification=binary_classification)\n",
    "test_labels, test_features, test_seq_lengths =\\\n",
    "    dp.prepare_labels_features_lengths(policy_histories=policy_histories_test,\n",
    "                                       policy_histories_lengths=policy_histories_length_test, \n",
    "                                       max_policy_history_length=max_length_policy_history,\n",
    "                                       binary_classification=binary_classification)\n",
    "\n",
    "train_data = dp.TrainDataSet(train_labels=train_labels, train_features=train_features, train_seq_lengths=train_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gan as gan\n",
    "\n",
    "\n",
    "learning_rate_g = 0.01\n",
    "learning_rate_d = 0.0001\n",
    "size_batch = 200\n",
    "n_inputs_g = 2\n",
    "n_outputs_g = 2\n",
    "n_inputs_d = 2\n",
    "n_outputs_d = 2 if binary_classification else max_length_policy_history\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "Z = tf.placeholder(tf.float32, [size_batch, max_length_policy_history, n_inputs_g], name='Z')\n",
    "seq_length_z = tf.placeholder(tf.int32, [None], name='seq_length_z')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, max_length_policy_history, n_inputs_d], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "seq_length_x = tf.placeholder(tf.int32, [None], name=\"seq_length_x\")\n",
    "\n",
    "Gz = gan.generator(Z=Z, n_outputs=n_outputs_g, seq_length=seq_length_z, leaky=False)\n",
    "\n",
    "Dx, y_pred_x = gan.discriminator(X=X, seq_length=seq_length_x, n_outputs=n_outputs_d, leaky=False)\n",
    "Dg, y_pred_g = gan.discriminator(X=Gz, seq_length=seq_length_z, n_outputs=n_outputs_d, leaky=False, reuse=True)\n",
    "\n",
    "loss_g, accuracy_g = gan.generator_loss(size_batch=size_batch, Dg=Dg)\n",
    "loss_real_d, accuracy_real_d = gan.discriminator_loss_real(Dx=Dx, y=y)\n",
    "loss_fake_d, accuracy_fake_d = gan.discriminator_loss_fake(size_batch=size_batch, Dg=Dg)\n",
    "\n",
    "g_trainer = gan.generator_trainer(learning_rate=learning_rate_g, loss=loss_g)\n",
    "d_trainer = gan.discriminator_trainer_real(learning_rate_d, loss_real_d + loss_fake_d)\n",
    "\n",
    "tf.summary.scalar('Generator_loss', loss_g)\n",
    "tf.summary.scalar('Discriminator_loss_real', loss_real_d)\n",
    "tf.summary.scalar('Discriminator_loss_fake', loss_fake_d)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "g_saver = tf.train.Saver(var_list=[var for var in tvars if \"t_generator\" in var.name])\n",
    "d_saver = tf.train.Saver(var_list=[var for var in tvars if \"t_discriminator\" in var.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': (lambda x: \"%.2f\" % x)})\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 lossRealD: 0.560439 accRealD: 0.935 lossFakeD: 0.542731 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 lossRealD: 0.531858 accRealD: 0.935 lossFakeD: 0.000115605 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 lossRealD: 1.96263 accRealD: 0.61 lossFakeD: 1.07759e-05 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 lossRealD: 0.724709 accRealD: 0.94 lossFakeD: 6.35264e-06 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 lossRealD: 0.266904 accRealD: 0.975 lossFakeD: 3.8576e-06 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 lossRealD: 1.12307 accRealD: 0.9 lossFakeD: 2.2012e-06 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 lossRealD: 0.890741 accRealD: 0.95 lossFakeD: 1.0711e-06 accFakeD: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 lossRealD: 0.275393 accRealD: 0.955 lossFakeD: 8.08835e-07 accFakeD: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9dc541a80ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         _, lossRealD, lossFakeD, accRealD, accFakeD =             sess.run([d_trainer, loss_real_d, loss_fake_d, accuracy_real_d, accuracy_fake_d],\n\u001b[1;32m     12\u001b[0m                      {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n\u001b[0;32m---> 13\u001b[0;31m                       Z: Z_batch, seq_length_z: seq_length_z_batch})\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     print(\"Epoch:\", epoch, \"lossRealD:\", lossRealD, \"accRealD:\", accRealD,\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "# Pre-train discriminator\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(train_data.num_examples // size_batch):\n",
    "        Z_batch = np.random.normal(0, 1, size=[size_batch, max_length_policy_history, n_inputs_g])\n",
    "        seq_length_z_batch = np.full(size_batch, max_length_policy_history)\n",
    "\n",
    "        y_batch, X_batch, seq_length_x_batch = train_data.next_batch(size_batch)\n",
    "\n",
    "        _, lossRealD, lossFakeD, accRealD, accFakeD = \\\n",
    "            sess.run([d_trainer, loss_real_d, loss_fake_d, accuracy_real_d, accuracy_fake_d],\n",
    "                     {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n",
    "                      Z: Z_batch, seq_length_z: seq_length_z_batch})\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"lossRealD:\", lossRealD, \"accRealD:\", accRealD,\n",
    "          \"lossFakeD:\", lossFakeD, \"accFakeD:\", accFakeD)\n",
    "\n",
    "d_saver.save(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"discriminator.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/gopora/MyStuff/Dev/Workspaces/Sandbox/TroubledLife/model_checkpoints/discriminator.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 lossRealD: 0.401523 accRealD: 0.98 lossFakeD: 0.55525 accFakeD: 1.0 lossG: 0.853194 accG: 0.0\n[[0.92 0.85]\n [0.90 1.62]\n [2.35 3.48]\n [1.43 3.55]\n [3.20 5.13]\n [1.59 4.29]\n [2.89 4.27]\n [1.22 3.11]\n [2.76 4.30]\n [1.47 3.74]\n [3.36 5.42]\n [1.66 4.46]\n [3.01 4.46]\n [1.26 3.27]\n [2.55 3.79]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 lossRealD: 0.0536714 accRealD: 0.995 lossFakeD: 0.753085 accFakeD: 0.13 lossG: 0.642449 accG: 0.87\n[[1.05 1.05]\n [1.68 2.39]\n [2.47 4.71]\n [4.44 7.41]\n [4.56 11.19]\n [9.22 16.12]\n [7.26 21.19]\n [15.45 28.07]\n [8.88 31.47]\n [20.06 37.97]\n [8.46 37.41]\n [19.94 39.19]\n [6.20 35.32]\n [16.33 32.85]\n [4.21 27.43]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 lossRealD: 0.0139686 accRealD: 0.99 lossFakeD: 0.308361 accFakeD: 1.0 lossG: 1.37213 accG: 0.0\n[[2.08 1.91]\n [4.99 5.87]\n [7.51 14.33]\n [9.75 18.11]\n [5.89 21.54]\n [5.98 17.48]\n [7.69 10.34]\n [9.88 19.12]\n [21.98 28.28]\n [33.65 56.65]\n [25.45 84.41]\n [29.99 74.73]\n [9.11 56.25]\n [8.81 32.35]\n [31.74 19.84]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 lossRealD: 0.050236 accRealD: 0.99 lossFakeD: 0.169732 accFakeD: 1.0 lossG: 2.27809 accG: 0.0\n[[3.49 3.65]\n [13.06 13.75]\n [26.03 17.72]\n [20.22 29.29]\n [24.76 42.09]\n [29.27 61.90]\n [24.21 77.49]\n [22.75 94.50]\n [37.48 111.93]\n [18.22 115.56]\n [34.83 123.48]\n [-9.43 30.06]\n [12.50 -10.28]\n [9.21 22.96]\n [1.15 26.38]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 lossRealD: 0.530823 accRealD: 0.95 lossFakeD: 0.290456 accFakeD: 1.0 lossG: 1.3894 accG: 0.0\n[[4.51 4.15]\n [8.44 11.16]\n [8.35 16.73]\n [9.05 20.50]\n [10.45 26.62]\n [12.21 31.67]\n [10.51 32.07]\n [14.58 36.33]\n [11.53 31.60]\n [12.52 30.75]\n [8.82 20.56]\n [9.54 23.05]\n [7.93 19.67]\n [8.45 21.37]\n [8.18 19.02]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e4cb52e46763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         _, lossRealD, accRealD =             sess.run([d_trainer, loss_real_d, accuracy_real_d],\n\u001b[1;32m     16\u001b[0m                      {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n\u001b[0;32m---> 17\u001b[0;31m                       Z: Z_batch, seq_length_z: seq_length_z_batch})\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyStuff/Dev/python-virtual-envs/pie36-tf13/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "d_saver.restore(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"discriminator.ckpt\"))\n",
    "#g_saver.restore(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"generator.ckpt\"))\n",
    "\n",
    "# Train generator and discriminator together\n",
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(train_data.num_examples // size_batch):\n",
    "        Z_batch = np.random.normal(0, 1, size=[size_batch, max_length_policy_history, n_inputs_g])\n",
    "        seq_length_z_batch = np.full(size_batch, max_length_policy_history)\n",
    "        \n",
    "        y_batch, X_batch, seq_length_x_batch = train_data.next_batch(size_batch)\n",
    "\n",
    "        # Train discriminator on both real and fake data\n",
    "        _, lossRealD, accRealD = \\\n",
    "            sess.run([d_trainer, loss_real_d, accuracy_real_d],\n",
    "                     {X: X_batch, y: y_batch, seq_length_x: seq_length_x_batch,\n",
    "                      Z: Z_batch, seq_length_z: seq_length_z_batch})\n",
    "\n",
    "        Z_batch = np.random.normal(0, 1, size=[size_batch, max_length_policy_history, n_inputs_g])\n",
    "\n",
    "        # Train generator    \n",
    "        _, lossFakeD, accFakeD, lossG, accG, gData = \\\n",
    "            sess.run([g_trainer, loss_fake_d, accuracy_fake_d, loss_g, accuracy_g, Gz], \n",
    "                     feed_dict={Z: Z_batch, seq_length_z: seq_length_z_batch})\n",
    "\n",
    "    print(\"Epoch:\", epoch, \n",
    "          \"lossRealD:\", lossRealD, \"accRealD:\", accRealD,\n",
    "          \"lossFakeD:\", lossFakeD, \"accFakeD:\", accFakeD,\n",
    "          \"lossG:\", lossG, \"accG:\", accG)\n",
    "\n",
    "    print(gData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gopora/MyStuff/Dev/Workspaces/Sandbox/TroubledLife/model_checkpoints/generator.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_saver.save(sess, os.path.join(MODEL_CHECKPOINTS_DIR, \"generator.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
